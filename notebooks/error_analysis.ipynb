{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the notebook\n",
    "\n",
    "\n",
    "This notebook performs error analysis for the best model for DataSolve 2022, and try to answer where and why it went wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score, multilabel_confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the best model artifact logged on Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_DIR = Path(\"../dbv3l\")\n",
    "if not os.path.exists(BEST_MODEL_DIR):\n",
    "    BEST_MODEL_DIR.mkdir()\n",
    "    api = wandb.Api()\n",
    "    for fold in range(5):\n",
    "        artifact = api.artifact(f\"gladiator/DataSolve-2022/dbv3l-15ep:v{fold}\", type=\"model\")\n",
    "        artifact.get_path(f\"oof_{fold}.pkl\").download(BEST_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all OOF dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train_folds_5.csv\")\n",
    "LABEL_COLS = [col for col in train_df.columns if col not in [\"id\", \"name\", \"document_text\", \"fold\"]]\n",
    "PRED_COLS = [f\"pred_{col}\" for col in LABEL_COLS]\n",
    "len(LABEL_COLS), len(PRED_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def post_process_logits(logits: np.ndarray, threshold=0.5):\n",
    "    probs = sigmoid(logits)\n",
    "    preds = np.zeros(probs.shape)\n",
    "    preds[np.where(probs >= threshold)] = 1\n",
    "    return preds\n",
    "\n",
    "oof_dfs = []\n",
    "for fold in range(5):\n",
    "    tmp_df = pd.DataFrame()\n",
    "    p = pd.read_pickle(BEST_MODEL_DIR/f'oof_{fold}.pkl')\n",
    "    tmp_df['id'] = p['id']\n",
    "    tmp_df[LABEL_COLS] = p['labels']\n",
    "    tmp_df[PRED_COLS] = p['logits']\n",
    "    oof_dfs.append(tmp_df)\n",
    "    \n",
    "oof_df = pd.concat(oof_dfs).sort_values(by='id').reset_index(drop=True)\n",
    "oof_df[PRED_COLS] = post_process_logits(oof_df[PRED_COLS]).astype(int)\n",
    "train_df = train_df.sort_values(by=\"id\")\n",
    "assert train_df.shape[0] == oof_df.shape[0]\n",
    "assert sum(oof_df[\"id\"].to_numpy() == train_df.id.to_numpy()) == train_df.shape[0]\n",
    "\n",
    "oof_df.insert(loc=1, column=\"name\", value=train_df[\"name\"].to_numpy())\n",
    "oof_df.insert(loc=2, column=\"document_text\", value=train_df[\"document_text\"].to_numpy())\n",
    "# rearrage columns for better visibility while analyses\n",
    "cols = []\n",
    "for col in LABEL_COLS:\n",
    "    cols.append(col)\n",
    "    cols.append(f\"pred_{col}\")\n",
    "oof_df = oof_df[[\"id\", \"name\", \"document_text\"] + cols]\n",
    "oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    oof_df[LABEL_COLS],\n",
    "    oof_df[PRED_COLS],\n",
    "    output_dict=False,\n",
    "    target_names=LABEL_COLS,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(\n",
    "        oof_df[LABEL_COLS],\n",
    "        oof_df[PRED_COLS],\n",
    "        output_dict=True,\n",
    "        target_names=LABEL_COLS,\n",
    "    )\n",
    "report_df = pd.DataFrame(report).T[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.insert(loc=0, column=\"class\", value=report_df.index)\n",
    "report_df = report_df[[\"class\", \"precision\", \"recall\", \"f1-score\", \"support\"]]\n",
    "report_df = report_df.reset_index(drop=True)\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log results as tables to W&B for interactive exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"DataSolve-2022\", group=\"error-analysis\", name=\"classification-report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log oof predictions and classification report as wandb table\n",
    "oof_table = wandb.Table(dataframe=oof_df)\n",
    "classification_report_table = wandb.Table(dataframe=report_df)\n",
    "\n",
    "run.log({\"oof_predictions\": oof_table, \"classification_report\": classification_report_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of support (i.e. average examples per classes)\n",
    "report_df[\"support\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df[\"support\"].min(),report_df[\"support\"].max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df[report_df[\"support\"] == 435.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df[report_df[\"support\"] == 1742.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = oof_df.iloc[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = tokenizer.cls_token + sample[\"name\"] + tokenizer.sep_token + sample[\"document_text\"] + tokenizer.sep_token\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer(sample_text, add_special_tokens=False)[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(sample_text, add_special_tokens=False)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def tokenize_func(example):\n",
    "    tok = tokenizer(example[\"text\"], add_special_tokens=False)\n",
    "    return {\"length\": len(tok[\"input_ids\"])}\n",
    "\n",
    "train_df[\"text\"] = tokenizer.cls_token + train_df[\"name\"] + tokenizer.sep_token + train_df[\"document_text\"] + tokenizer.sep_token\n",
    "ds = Dataset.from_pandas(train_df)\n",
    "ds = ds.map(tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.length.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len_df = pd.DataFrame()\n",
    "\n",
    "means = []\n",
    "maxis = []\n",
    "minis = []\n",
    "total_samples = []\n",
    "for col in LABEL_COLS:\n",
    "    mean = int(df[df[col] == 1][\"length\"].mean())\n",
    "    maxi = df[df[col] == 1][\"length\"].max()\n",
    "    mini = df[df[col] == 1][\"length\"].min()\n",
    "    total_sample = len(df[df[col] == 1])\n",
    "    means.append(mean); maxis.append(maxi); minis.append(mini)\n",
    "    total_samples.append(total_sample)\n",
    "\n",
    "len_df[\"label\"] = LABEL_COLS\n",
    "len_df[\"mean\"] = means\n",
    "len_df[\"max\"] = maxis\n",
    "len_df[\"min\"] = minis\n",
    "len_df[\"total_samples\"] = total_samples\n",
    "len_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df[\"total_samples\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

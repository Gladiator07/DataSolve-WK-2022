{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation trials for multi-label classification. Using deberta-v3-base as the student model and deberta-v3-large as the teacher model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.012442,
     "end_time": "2022-11-19T20:36:20.42823",
     "exception": false,
     "start_time": "2022-11-19T20:36:20.415788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.031254,
     "end_time": "2022-11-19T20:36:20.46981",
     "exception": false,
     "start_time": "2022-11-19T20:36:20.438556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "WANDB = False\n",
    "ENVIRON = \"jarvislabs\"\n",
    "NOTES = \"knowledge distillation on deberta-v3-base with trained deberta-v3-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009283,
     "end_time": "2022-11-19T20:36:20.488055",
     "exception": false,
     "start_time": "2022-11-19T20:36:20.478772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 60.824307,
     "end_time": "2022-11-19T20:37:21.31941",
     "exception": false,
     "start_time": "2022-11-19T20:36:20.495103",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT = \"DataSolve-2022\"\n",
    "\n",
    "if ENVIRON == \"jarvislabs\":\n",
    "    ROOT_DIR = Path(f\"/home/{PROJECT}\")\n",
    "    ARTIFACTS_DIR = Path(\"/home/artifacts\")\n",
    "    SETUP_SCRIPT_PATH = Path(\"/home/setup.sh\")\n",
    "elif ENVIRON == \"lambdalabs\":\n",
    "    ROOT_DIR = Path(f\"/ubuntu/home/{PROJECT}\")\n",
    "    ARTIFACTS_DIR = Path(\"/ubuntu/home/artifacts\")\n",
    "    SETUP_SCRIPT_PATH = Path(\"/home/setup.sh\")\n",
    "elif ENVIRON == \"kaggle\":\n",
    "    ROOT_DIR = Path(f\"/kaggle/working/{PROJECT}\")\n",
    "    ARTIFACTS_DIR = Path(\"/kaggle/working/artifacts\")\n",
    "    SETUP_SCRIPT_PATH = Path(\"/kaggle/input/datasolve-setup-script/setup.sh\")\n",
    "    \n",
    "if not pkgutil.find_loader(\"omegaconf\") and ENVIRON == \"kaggle\":\n",
    "    !bash {SETUP_SCRIPT_PATH} {ENVIRON} \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.036363,
     "end_time": "2022-11-19T20:37:21.367118",
     "exception": false,
     "start_time": "2022-11-19T20:37:21.330755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load secret keys\n",
    "%load_ext dotenv\n",
    "%dotenv {ROOT_DIR}/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD TEACHER MODEL\n",
    "import os, wandb\n",
    "experiment = \"crimson-elevator-29\"\n",
    "if not os.path.exists(f\"./{experiment}\"):\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(f\"gladiator/DataSolve-2022/{experiment}:v0\", type=\"model\")\n",
    "    artifact_dir = artifact.download(f\"./{experiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010899,
     "end_time": "2022-11-19T20:37:21.389126",
     "exception": false,
     "start_time": "2022-11-19T20:37:21.378227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.092095,
     "end_time": "2022-11-19T20:37:21.492386",
     "exception": false,
     "start_time": "2022-11-19T20:37:21.400291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: false\n",
      "wandb: false\n",
      "seed: 42\n",
      "train_csv: train_processed.csv\n",
      "model:\n",
      "  model_name_or_path: microsoft/deberta-v3-large\n",
      "  gradient_checkpointing: true\n",
      "  output_hidden_states: false\n",
      "  output_last_hidden_state: false\n",
      "  output_pooled_embeds: false\n",
      "teacher_model_path: crimson-elevator-29/pytorch_model.bin\n",
      "tags:\n",
      "- clspool\n",
      "- microsoft/deberta-v3-large\n",
      "- '512'\n",
      "- tts_split\n",
      "notes: knowledge distillation on deberta-v3-base with trained deberta-v3-large\n",
      "upload_artifacts_to_wandb: true\n",
      "data:\n",
      "  max_length: 512\n",
      "  truncation: true\n",
      "  pad_to_multiple_of: 8\n",
      "training_args:\n",
      "  seed: 42\n",
      "  evaluation_strategy: epoch\n",
      "  save_strategy: epoch\n",
      "  save_total_limit: 1\n",
      "  num_train_epochs: 8\n",
      "  lr_scheduler_type: cosine\n",
      "  warmup_ratio: 0.2\n",
      "  per_device_train_batch_size: 16\n",
      "  per_device_eval_batch_size: 16\n",
      "  gradient_accumulation_steps: 1\n",
      "  learning_rate: 5.0e-05\n",
      "  weight_decay: 0.01\n",
      "  max_grad_norm: 1.0\n",
      "  adam_epsilon: 1.0e-06\n",
      "  fp16: true\n",
      "  dataloader_num_workers: 6\n",
      "  load_best_model_at_end: true\n",
      "  metric_for_best_model: eval_f1\n",
      "  greater_is_better: true\n",
      "  group_by_length: true\n",
      "  length_column_name: length\n",
      "  report_to: none\n",
      "  dataloader_pin_memory: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "gc.enable()\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "class Config:\n",
    "    # GENERAL\n",
    "    debug = DEBUG\n",
    "    wandb = WANDB\n",
    "    seed = 42\n",
    "    train_csv = \"train_processed.csv\"\n",
    "    \n",
    "    # MODEL\n",
    "    model = dict(\n",
    "        model_name_or_path = \"microsoft/deberta-v3-large\",\n",
    "        gradient_checkpointing = True,\n",
    "        output_hidden_states = False,\n",
    "        output_last_hidden_state = False,\n",
    "        output_pooled_embeds = False\n",
    "    \n",
    "    )\n",
    "    teacher_model_path = \"crimson-elevator-29/pytorch_model.bin\"\n",
    "\n",
    "    # TRACKING\n",
    "    tags = [\"clspool\", f\"{model['model_name_or_path']}\", \"512\", \"tts_split\"]\n",
    "    notes = NOTES\n",
    "    upload_artifacts_to_wandb = True\n",
    "    \n",
    "    # DATA\n",
    "    data = dict(\n",
    "        max_length = 512,\n",
    "        truncation = True,\n",
    "        pad_to_multiple_of = 8,\n",
    "    )\n",
    "    \n",
    "    # TRAINING ARGUMENTS\n",
    "    training_args = dict(\n",
    "        # general\n",
    "        seed = seed,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        save_total_limit = 1,\n",
    "\n",
    "        # train settings\n",
    "        num_train_epochs = 8,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        warmup_ratio = 0.2,\n",
    "        per_device_train_batch_size = 16,\n",
    "        per_device_eval_batch_size = 16,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        learning_rate = 5e-5,\n",
    "        weight_decay = 0.01,\n",
    "        max_grad_norm = 1.0,\n",
    "        \n",
    "        # misc\n",
    "        # eval_accumulation_steps=10,\n",
    "        adam_epsilon = 1e-6,\n",
    "        fp16 = True,\n",
    "        dataloader_num_workers = min(6, os.cpu_count()),\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_f1\",\n",
    "        greater_is_better = True,\n",
    "        group_by_length = True,\n",
    "        length_column_name = \"length\",\n",
    "        report_to = \"wandb\" if WANDB else \"none\",\n",
    "        dataloader_pin_memory = True,\n",
    "    )\n",
    "\n",
    "\n",
    "# CONFIG SETTINGS\n",
    "config_dict = {x:dict(Config.__dict__)[x] for x in dict(Config.__dict__) if not x.startswith('_')}\n",
    "cfg = OmegaConf.create(config_dict)\n",
    "if cfg.debug: cfg.tags += [\"debug\"]\n",
    "if cfg.debug: cfg.training_args.num_train_epochs = 2\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010949,
     "end_time": "2022-11-19T20:37:21.514983",
     "exception": false,
     "start_time": "2022-11-19T20:37:21.504034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 9.61634,
     "end_time": "2022-11-19T20:37:31.14242",
     "exception": false,
     "start_time": "2022-11-19T20:37:21.52608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import shutil\n",
    "import pickle\n",
    "import warnings\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Callable, Optional\n",
    "\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datasets, transformers\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    EvalPrediction,\n",
    "    PreTrainedTokenizer,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "\n",
    "# SYSTEM SETTINGS\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "os.environ[\"WANDB_SILENT\"] = \"false\"\n",
    "set_seed(cfg.seed)\n",
    "if not cfg.debug:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011772,
     "end_time": "2022-11-19T20:37:31.166461",
     "exception": false,
     "start_time": "2022-11-19T20:37:31.154689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.022708,
     "end_time": "2022-11-19T20:37:31.20094",
     "exception": false,
     "start_time": "2022-11-19T20:37:31.178232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_checkpoints(dir):\n",
    "    for file in glob.glob(f\"{dir}/checkpoint-*\"):\n",
    "        shutil.rmtree(file, ignore_errors=True)\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def delete_file(path: str):\n",
    "    if os.exists(path):\n",
    "        os.remove(path)\n",
    "\n",
    "def save_pickle(obj, filepath):\n",
    "    with open(filepath, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "def process_hydra_config_for_wandb(cfg: OmegaConf):\n",
    "    \"\"\"\n",
    "    Only keep relevant part of config for logging\n",
    "    \"\"\"\n",
    "    tmp_cfg = copy.deepcopy(cfg)\n",
    "    cfg_dict = OmegaConf.to_container(tmp_cfg, resolve=True, throw_on_missing=True)\n",
    "    del cfg_dict[\"training_args\"]\n",
    "    return cfg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init W&B run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.wandb:\n",
    "    wandb.init(\n",
    "        project=\"DataSolve-2022\",\n",
    "        tags=cfg.tags,\n",
    "        notes=cfg.notes,\n",
    "        config=process_hydra_config_for_wandb(cfg),\n",
    "        save_code=True,\n",
    "    )\n",
    "    wandb.alert(\n",
    "        title=f\"Experiment {wandb.run.name}\",\n",
    "        text=f\"ðŸš€ Starting experiment {wandb.run.name}, Description: {cfg.notes}\",\n",
    "        level=AlertLevel.INFO,\n",
    "        wait_duration=0,\n",
    "    )\n",
    "\n",
    "EXP_NAME = wandb.run.name if cfg.wandb else \"debug\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011751,
     "end_time": "2022-11-19T20:37:31.224629",
     "exception": false,
     "start_time": "2022-11-19T20:37:31.212878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.252636,
     "end_time": "2022-11-19T20:37:31.489092",
     "exception": false,
     "start_time": "2022-11-19T20:37:31.236456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>document_text</th>\n",
       "      <th>Accounting and Finance</th>\n",
       "      <th>Antitrust</th>\n",
       "      <th>Banking</th>\n",
       "      <th>Broker Dealer</th>\n",
       "      <th>Commodities Trading</th>\n",
       "      <th>Compliance Management</th>\n",
       "      <th>Consumer protection</th>\n",
       "      <th>...</th>\n",
       "      <th>Required Disclosures</th>\n",
       "      <th>Research</th>\n",
       "      <th>Risk Management</th>\n",
       "      <th>Securities Clearing</th>\n",
       "      <th>Securities Issuing</th>\n",
       "      <th>Securities Management</th>\n",
       "      <th>Securities Sales</th>\n",
       "      <th>Securities Settlement</th>\n",
       "      <th>Trade Pricing</th>\n",
       "      <th>Trade Settlement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4772</td>\n",
       "      <td>Consent Order in the Matter of Solium Financia...</td>\n",
       "      <td>Solium Financial Services LLC (\"SFS\") is a bro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4774</td>\n",
       "      <td>Alberta Securities Commission Warns Investors ...</td>\n",
       "      <td>A new year brings new investment opportunities...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4775</td>\n",
       "      <td>Exempt Market Dealer Agrees to Settlement</td>\n",
       "      <td>The Alberta Securities Commission (ASC) has co...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4776</td>\n",
       "      <td>Canadian Securities Regulators Announces Consu...</td>\n",
       "      <td>The Canadian Securities Administrators (CSA) p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4778</td>\n",
       "      <td>CSA Consultation Paper 51-405 Consideration of...</td>\n",
       "      <td>On April 6, 2017, the Canadian Securities Admi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               name  \\\n",
       "0  4772  Consent Order in the Matter of Solium Financia...   \n",
       "1  4774  Alberta Securities Commission Warns Investors ...   \n",
       "2  4775          Exempt Market Dealer Agrees to Settlement   \n",
       "3  4776  Canadian Securities Regulators Announces Consu...   \n",
       "4  4778  CSA Consultation Paper 51-405 Consideration of...   \n",
       "\n",
       "                                       document_text  Accounting and Finance  \\\n",
       "0  Solium Financial Services LLC (\"SFS\") is a bro...                       0   \n",
       "1  A new year brings new investment opportunities...                       0   \n",
       "2  The Alberta Securities Commission (ASC) has co...                       0   \n",
       "3  The Canadian Securities Administrators (CSA) p...                       0   \n",
       "4  On April 6, 2017, the Canadian Securities Admi...                       0   \n",
       "\n",
       "   Antitrust  Banking  Broker Dealer  Commodities Trading  \\\n",
       "0          0        0              1                    0   \n",
       "1          0        0              0                    0   \n",
       "2          0        0              1                    0   \n",
       "3          0        0              0                    0   \n",
       "4          0        0              0                    0   \n",
       "\n",
       "   Compliance Management  Consumer protection  ...  Required Disclosures  \\\n",
       "0                      1                    0  ...                     0   \n",
       "1                      0                    0  ...                     0   \n",
       "2                      1                    0  ...                     0   \n",
       "3                      0                    0  ...                     1   \n",
       "4                      0                    1  ...                     1   \n",
       "\n",
       "   Research  Risk Management  Securities Clearing  Securities Issuing  \\\n",
       "0         0                0                    0                   0   \n",
       "1         0                0                    0                   0   \n",
       "2         0                0                    0                   0   \n",
       "3         0                0                    0                   0   \n",
       "4         0                0                    0                   0   \n",
       "\n",
       "   Securities Management  Securities Sales  Securities Settlement  \\\n",
       "0                      0                 0                      0   \n",
       "1                      0                 0                      0   \n",
       "2                      0                 1                      1   \n",
       "3                      0                 0                      1   \n",
       "4                      0                 0                      1   \n",
       "\n",
       "   Trade Pricing  Trade Settlement  \n",
       "0              0                 0  \n",
       "1              0                 0  \n",
       "2              0                 1  \n",
       "3              0                 0  \n",
       "4              0                 0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ DATA\n",
    "df = pd.read_csv(ROOT_DIR/'input'/cfg.train_csv)\n",
    "if cfg.debug:\n",
    "    df = df.sample(100, random_state=42).reset_index(drop=True)\n",
    "LABEL_COLS = [col for col in df.columns if col not in [\"id\", \"name\", \"document_text\"]]\n",
    "print(len(LABEL_COLS))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.234843,
     "end_time": "2022-11-19T20:37:31.738067",
     "exception": false,
     "start_time": "2022-11-19T20:37:31.503224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7885\n",
      "1974\n"
     ]
    }
   ],
   "source": [
    "X = df[[col for col in df.columns if col not in LABEL_COLS]]\n",
    "y = df[LABEL_COLS]\n",
    "\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, val_index in msss.split(X, y):\n",
    "    print(len(train_index))\n",
    "    print(len(val_index))\n",
    "    val_df = df.loc[val_index]\n",
    "    train_df = df.loc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.023409,
     "end_time": "2022-11-19T20:37:31.774411",
     "exception": false,
     "start_time": "2022-11-19T20:37:31.751002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7885, 53), (1974, 53))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 24.305877,
     "end_time": "2022-11-19T20:37:56.092882",
     "exception": false,
     "start_time": "2022-11-19T20:37:31.787005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d09ab7634541babf106c4cf60b5c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/3943 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66feade182e54e71a32617d2f2b3379c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3942 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1515b37d231474bbcff1898588e02a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/987 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdd758a066b47ab8160d61a780f4211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/987 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7885 1974\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.model_name_or_path)\n",
    "\n",
    "def tokenize_func(example: pd.Series, tokenizer: PreTrainedTokenizer, max_length: int = 512, truncation: bool = True, mode: str=\"train\"):\n",
    "    tokenized = tokenizer(\n",
    "    example[\"text\"],\n",
    "    truncation=truncation,\n",
    "    max_length=max_length,\n",
    "    add_special_tokens=True,\n",
    ")\n",
    "    if mode == \"train\":\n",
    "        tokenized[\"labels\"] = [example[i] for i in LABEL_COLS]\n",
    "    tokenized[\"length\"] = len(tokenized[\"input_ids\"])\n",
    "    return tokenized\n",
    "\n",
    "def preprocess_data(df_: pd.DataFrame, mode:str=\"train\"):\n",
    "    df_[\"text\"] = tokenizer.cls_token + df_[\"name\"] + tokenizer.sep_token + df_[\"document_text\"] + tokenizer.sep_token\n",
    "    tok_ds = Dataset.from_pandas(df_)\n",
    "    tok_ds = tok_ds.map(lambda x: tokenize_func(x, tokenizer, max_length=cfg.data.max_length, truncation=cfg.data.truncation, mode=mode), num_proc=2)\n",
    "    return tok_ds\n",
    "\n",
    "train_ds = preprocess_data(train_df, mode=\"train\")\n",
    "val_ds = preprocess_data(val_df, mode=\"train\")\n",
    "print(len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.03757,
     "end_time": "2022-11-19T20:37:56.144422",
     "exception": false,
     "start_time": "2022-11-19T20:37:56.106852",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4772,\n",
       " 'name': 'Consent Order in the Matter of Solium Financial Services LLC',\n",
       " 'document_text': 'Solium Financial Services LLC (\"SFS\") is a broker-dealer with a principal place of business at 50 Tice Boulevard, Suite A-18 Woodcliff Lake, New Jersey 07677, and is registered as a broker-dealer with the Alabama Securities Commission (\"Commission\"). During the period from at least January 2009 to June 6, 2019, SFS acted as broker-dealer in Alabama as the term broker-dealer is defined by Title 8, Chapter 6, 8-6-2 of the Act. Code of Alabama, 8-6-3(a) states that it is unlawful for a person to transact business in Alabama as a broker-dealer or agent unless such person is registered under the Act. By engaging in the conduct set forth above, SFS acted as an unregistered broker-dealer in Alabama in violation of 8-6-3(a) of the Act. This Order concludes the investigation by the Commission and any other action that the Commission could commence under applicable Alabama law as it relates to the substance of the Findings of Fact and Conclusions of Law herein, provided, however, that the Commission may pursue claims arising from SFS failure to comply with the terms of this Order. The fine, in this case, is $7,624.01.',\n",
       " 'Accounting and Finance': 0,\n",
       " 'Antitrust': 0,\n",
       " 'Banking': 0,\n",
       " 'Broker Dealer': 1,\n",
       " 'Commodities Trading': 0,\n",
       " 'Compliance Management': 1,\n",
       " 'Consumer protection': 0,\n",
       " 'Contract Provisions': 0,\n",
       " 'Corporate Communications': 0,\n",
       " 'Corporate Governance': 0,\n",
       " 'Definitions': 1,\n",
       " 'Delivery': 0,\n",
       " 'Examinations': 0,\n",
       " 'Exemptions': 0,\n",
       " 'Fees and Charges': 0,\n",
       " 'Financial Accounting': 0,\n",
       " 'Financial Crime': 0,\n",
       " 'Forms': 0,\n",
       " 'Fraud': 0,\n",
       " 'IT Risk': 0,\n",
       " 'Information Filing': 0,\n",
       " 'Insurance': 0,\n",
       " 'Legal': 1,\n",
       " 'Legal Proceedings': 1,\n",
       " 'Licensing': 1,\n",
       " 'Licensure and certification': 1,\n",
       " 'Liquidity Risk': 0,\n",
       " 'Listing': 0,\n",
       " 'Market Abuse': 0,\n",
       " 'Market Risk': 0,\n",
       " 'Monetary and Economic Policy': 0,\n",
       " 'Money Services': 0,\n",
       " 'Money-Laundering and Terrorist Financing': 0,\n",
       " 'Natural Disasters': 0,\n",
       " 'Payments and Settlements': 0,\n",
       " 'Powers and Duties': 0,\n",
       " 'Quotation': 0,\n",
       " 'Records Maintenance': 0,\n",
       " 'Regulatory Actions': 0,\n",
       " 'Regulatory Reporting': 0,\n",
       " 'Required Disclosures': 0,\n",
       " 'Research': 0,\n",
       " 'Risk Management': 0,\n",
       " 'Securities Clearing': 0,\n",
       " 'Securities Issuing': 0,\n",
       " 'Securities Management': 0,\n",
       " 'Securities Sales': 0,\n",
       " 'Securities Settlement': 0,\n",
       " 'Trade Pricing': 0,\n",
       " 'Trade Settlement': 0,\n",
       " 'text': '[CLS]Consent Order in the Matter of Solium Financial Services LLC[SEP]Solium Financial Services LLC (\"SFS\") is a broker-dealer with a principal place of business at 50 Tice Boulevard, Suite A-18 Woodcliff Lake, New Jersey 07677, and is registered as a broker-dealer with the Alabama Securities Commission (\"Commission\"). During the period from at least January 2009 to June 6, 2019, SFS acted as broker-dealer in Alabama as the term broker-dealer is defined by Title 8, Chapter 6, 8-6-2 of the Act. Code of Alabama, 8-6-3(a) states that it is unlawful for a person to transact business in Alabama as a broker-dealer or agent unless such person is registered under the Act. By engaging in the conduct set forth above, SFS acted as an unregistered broker-dealer in Alabama in violation of 8-6-3(a) of the Act. This Order concludes the investigation by the Commission and any other action that the Commission could commence under applicable Alabama law as it relates to the substance of the Findings of Fact and Conclusions of Law herein, provided, however, that the Commission may pursue claims arising from SFS failure to comply with the terms of this Order. The fine, in this case, is $7,624.01.[SEP]',\n",
       " '__index_level_0__': 0,\n",
       " 'input_ids': [1,\n",
       "  1,\n",
       "  36219,\n",
       "  4077,\n",
       "  267,\n",
       "  262,\n",
       "  14759,\n",
       "  265,\n",
       "  471,\n",
       "  60661,\n",
       "  3729,\n",
       "  1724,\n",
       "  3927,\n",
       "  2,\n",
       "  471,\n",
       "  60661,\n",
       "  3729,\n",
       "  1724,\n",
       "  3927,\n",
       "  287,\n",
       "  309,\n",
       "  430,\n",
       "  16480,\n",
       "  309,\n",
       "  285,\n",
       "  269,\n",
       "  266,\n",
       "  7347,\n",
       "  271,\n",
       "  58161,\n",
       "  275,\n",
       "  266,\n",
       "  4891,\n",
       "  470,\n",
       "  265,\n",
       "  460,\n",
       "  288,\n",
       "  960,\n",
       "  897,\n",
       "  10953,\n",
       "  14201,\n",
       "  261,\n",
       "  9500,\n",
       "  336,\n",
       "  271,\n",
       "  2048,\n",
       "  4059,\n",
       "  40099,\n",
       "  2202,\n",
       "  261,\n",
       "  485,\n",
       "  3744,\n",
       "  7844,\n",
       "  46692,\n",
       "  261,\n",
       "  263,\n",
       "  269,\n",
       "  2079,\n",
       "  283,\n",
       "  266,\n",
       "  7347,\n",
       "  271,\n",
       "  58161,\n",
       "  275,\n",
       "  262,\n",
       "  6002,\n",
       "  10207,\n",
       "  2653,\n",
       "  287,\n",
       "  309,\n",
       "  75573,\n",
       "  309,\n",
       "  285,\n",
       "  260,\n",
       "  1717,\n",
       "  262,\n",
       "  926,\n",
       "  292,\n",
       "  288,\n",
       "  668,\n",
       "  1278,\n",
       "  1812,\n",
       "  264,\n",
       "  1172,\n",
       "  525,\n",
       "  261,\n",
       "  1112,\n",
       "  261,\n",
       "  10000,\n",
       "  430,\n",
       "  8736,\n",
       "  283,\n",
       "  7347,\n",
       "  271,\n",
       "  58161,\n",
       "  267,\n",
       "  6002,\n",
       "  283,\n",
       "  262,\n",
       "  1384,\n",
       "  7347,\n",
       "  271,\n",
       "  58161,\n",
       "  269,\n",
       "  3034,\n",
       "  293,\n",
       "  7181,\n",
       "  578,\n",
       "  261,\n",
       "  4696,\n",
       "  525,\n",
       "  261,\n",
       "  578,\n",
       "  271,\n",
       "  765,\n",
       "  271,\n",
       "  445,\n",
       "  265,\n",
       "  262,\n",
       "  1878,\n",
       "  260,\n",
       "  3506,\n",
       "  265,\n",
       "  6002,\n",
       "  261,\n",
       "  578,\n",
       "  271,\n",
       "  765,\n",
       "  271,\n",
       "  508,\n",
       "  555,\n",
       "  452,\n",
       "  285,\n",
       "  1603,\n",
       "  272,\n",
       "  278,\n",
       "  269,\n",
       "  15082,\n",
       "  270,\n",
       "  266,\n",
       "  604,\n",
       "  264,\n",
       "  51494,\n",
       "  460,\n",
       "  267,\n",
       "  6002,\n",
       "  283,\n",
       "  266,\n",
       "  7347,\n",
       "  271,\n",
       "  58161,\n",
       "  289,\n",
       "  2645,\n",
       "  2336,\n",
       "  405,\n",
       "  604,\n",
       "  269,\n",
       "  2079,\n",
       "  494,\n",
       "  262,\n",
       "  1878,\n",
       "  260,\n",
       "  927,\n",
       "  4686,\n",
       "  267,\n",
       "  262,\n",
       "  3360,\n",
       "  487,\n",
       "  4243,\n",
       "  764,\n",
       "  261,\n",
       "  10000,\n",
       "  430,\n",
       "  8736,\n",
       "  283,\n",
       "  299,\n",
       "  46245,\n",
       "  7347,\n",
       "  271,\n",
       "  58161,\n",
       "  267,\n",
       "  6002,\n",
       "  267,\n",
       "  6527,\n",
       "  265,\n",
       "  578,\n",
       "  271,\n",
       "  765,\n",
       "  271,\n",
       "  508,\n",
       "  555,\n",
       "  452,\n",
       "  285,\n",
       "  265,\n",
       "  262,\n",
       "  1878,\n",
       "  260,\n",
       "  329,\n",
       "  4077,\n",
       "  14207,\n",
       "  262,\n",
       "  2485,\n",
       "  293,\n",
       "  262,\n",
       "  2653,\n",
       "  263,\n",
       "  356,\n",
       "  340,\n",
       "  1016,\n",
       "  272,\n",
       "  262,\n",
       "  2653,\n",
       "  387,\n",
       "  15851,\n",
       "  494,\n",
       "  4190,\n",
       "  6002,\n",
       "  818,\n",
       "  283,\n",
       "  278,\n",
       "  9611,\n",
       "  264,\n",
       "  262,\n",
       "  5182,\n",
       "  265,\n",
       "  262,\n",
       "  38269,\n",
       "  265,\n",
       "  17696,\n",
       "  263,\n",
       "  42068,\n",
       "  265,\n",
       "  2321,\n",
       "  10349,\n",
       "  261,\n",
       "  949,\n",
       "  261,\n",
       "  901,\n",
       "  261,\n",
       "  272,\n",
       "  262,\n",
       "  2653,\n",
       "  372,\n",
       "  5012,\n",
       "  2071,\n",
       "  9264,\n",
       "  292,\n",
       "  10000,\n",
       "  430,\n",
       "  2694,\n",
       "  264,\n",
       "  6378,\n",
       "  275,\n",
       "  262,\n",
       "  1169,\n",
       "  265,\n",
       "  291,\n",
       "  4077,\n",
       "  260,\n",
       "  279,\n",
       "  1399,\n",
       "  261,\n",
       "  267,\n",
       "  291,\n",
       "  571,\n",
       "  261,\n",
       "  269,\n",
       "  419,\n",
       "  819,\n",
       "  261,\n",
       "  48328,\n",
       "  260,\n",
       "  3085,\n",
       "  260,\n",
       "  2,\n",
       "  2],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'length': 276}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013486,
     "end_time": "2022-11-19T20:37:56.1719",
     "exception": false,
     "start_time": "2022-11-19T20:37:56.158414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.024145,
     "end_time": "2022-11-19T20:37:56.209778",
     "exception": false,
     "start_time": "2022-11-19T20:37:56.185633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_process_logits(logits: np.ndarray, threshold=0.5):\n",
    "    # first, apply sigmoid on logits which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(logits))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    preds = np.zeros(probs.shape)\n",
    "    preds[np.where(probs >= threshold)] = 1\n",
    "    preds = preds.flatten().astype(int)\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # `predictions` might return last_hidden_state or pooled_embeds\n",
    "    # In that case, take the first element (array) of the tuple for logits\n",
    "    logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = post_process_logits(logits)\n",
    "    labels = p.label_ids.flatten()\n",
    "    f1_macro_average = f1_score(labels, preds, average='macro')\n",
    "    roc_auc = roc_auc_score(labels, preds, average = 'macro')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    # return as dictionary\n",
    "    return {\n",
    "        'f1': f1_macro_average,\n",
    "        'roc_auc': roc_auc,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013594,
     "end_time": "2022-11-19T20:37:56.237049",
     "exception": false,
     "start_time": "2022-11-19T20:37:56.223455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.021072,
     "end_time": "2022-11-19T20:37:56.271589",
     "exception": false,
     "start_time": "2022-11-19T20:37:56.250517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs_stu = model(**inputs)\n",
    "        # extract bce loss and logits from student\n",
    "        loss_stu = outputs_stu.loss\n",
    "        logits_stu = outputs_stu.logits\n",
    "        # extract logits from teacher\n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher_model(**inputs)\n",
    "            loss_tea = outputs_tea.loss\n",
    "            logits_tea = outputs_tea.logits\n",
    "        # soften probabilites and compute distillation loss\n",
    "        # kdl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        # temperature\n",
    "        # logits_stu = torch.sigmoid(logits_stu) / self.args.temperature\n",
    "        # logits_tea = torch.sigmoid(logits_tea) / self.args.temperature\n",
    "        \n",
    "        # loss_kd = self.args.temperature ** 2 \\\n",
    "                    # * sum(kdl_loss(logits_stu[idx], logits_tea[idx]) for idx in range(50)) \n",
    "        # return weighted student loss\n",
    "        # loss = self.args.alpha * loss_stu + (1. - self.args.alpha) * loss_kd\n",
    "        # Compute losses\n",
    "        distillation_loss = nn.BCEWithLogitsLoss()(logits_tea.view(-1, 50),logits_stu.view(-1, 50))\n",
    "        loss = self.args.alpha * loss_stu + (1. - self.args.alpha) * distillation_loss\n",
    "        return (loss, outputs_stu) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013631,
     "end_time": "2022-11-19T20:37:56.298921",
     "exception": false,
     "start_time": "2022-11-19T20:37:56.28529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 0.0286,
     "end_time": "2022-11-19T20:37:56.341355",
     "exception": false,
     "start_time": "2022-11-19T20:37:56.312755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CustomModelOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    pooled_embeds: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_name_or_path: str\n",
    "    gradient_checkpointing: Optional[bool] = False\n",
    "    output_hidden_states: Optional[bool] = False\n",
    "    output_last_hidden_state: Optional[bool] = False\n",
    "    output_pooled_embeds: Optional[bool] = False\n",
    "\n",
    "class DataSolveModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.model_config = config\n",
    "        self.hf_config = AutoConfig.from_pretrained(self.model_config.model_name_or_path)\n",
    "        self.backbone = AutoModel.from_pretrained(self.model_config.model_name_or_path, config=self.hf_config)\n",
    "        if self.model_config.gradient_checkpointing:\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "        self.output = nn.Linear(self.hf_config.hidden_size, 50)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        trans_out = self.backbone(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = trans_out.last_hidden_state\n",
    "        pooled_embeds = last_hidden_state[:, 0]  # CLS token\n",
    "        logits = self.output(pooled_embeds)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, 50), \n",
    "                            labels.float().view(-1, 50))\n",
    "\n",
    "        return CustomModelOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=trans_out.hidden_states if self.model_config.output_hidden_states else None,\n",
    "            last_hidden_state=last_hidden_state if self.model_config.output_last_hidden_state else None,\n",
    "            pooled_embeds=pooled_embeds if self.model_config.output_pooled_embeds else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013329,
     "end_time": "2022-11-19T20:37:56.368443",
     "exception": false,
     "start_time": "2022-11-19T20:37:56.355114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 15677.599997,
     "end_time": "2022-11-20T00:59:13.982221",
     "exception": false,
     "start_time": "2022-11-19T20:37:56.382224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving outputs to /home/artifacts/debug\n",
      "EXPERIMENT: debug, DESC: knowledge distillation on deberta-v3-base with trained deberta-v3-large\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='3944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  65/3944 02:00 < 2:03:44, 0.52 it/s, Epoch 0.13/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed(cfg.seed)\n",
    "\n",
    "OUT_DIR = Path(ARTIFACTS_DIR/f'{EXP_NAME}')\n",
    "print(f\"Saving outputs to {OUT_DIR}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(f\"EXPERIMENT: {EXP_NAME}, DESC: {cfg.notes}\\n\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# sort by length to have similar length samples in each batch for speeding up evaluation\n",
    "val_ds = val_ds.sort(\"length\")\n",
    "\n",
    "# remove unwanted columns\n",
    "keep_cols = {\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"}\n",
    "remove_cols = [c for c in train_ds.column_names if c not in keep_cols]\n",
    "train_ds = train_ds.remove_columns(remove_cols)\n",
    "val_ds = val_ds.remove_columns(remove_cols)\n",
    "train_ds.set_format(\"torch\")\n",
    "val_ds.set_format(\"torch\")\n",
    "\n",
    "# init model\n",
    "teacher_model_config = ModelConfig(\"microsoft/deberta-v3-large\")\n",
    "teacher_model = DataSolveModel(teacher_model_config)\n",
    "teacher_model.load_state_dict(torch.load(cfg.teacher_model_path))\n",
    "teacher_model.to(torch.device(\"cuda\"))\n",
    "teacher_model.eval()\n",
    "\n",
    "model_config = ModelConfig(**cfg.model)\n",
    "model = DataSolveModel(model_config)\n",
    "\n",
    "# init trainer\n",
    "training_args = DistillationTrainingArguments(output_dir=OUT_DIR, **cfg.training_args)\n",
    "trainer = DistillationTrainer(\n",
    "            model,\n",
    "            teacher_model=teacher_model,\n",
    "            args=training_args,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=cfg.data.pad_to_multiple_of),\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    ")\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# ---------------------------------- Save, log, cleanup and upload ---------------------\n",
    "\n",
    "# Save model\n",
    "delete_checkpoints(OUT_DIR)\n",
    "trainer.save_model()\n",
    "clear_memory()\n",
    "\n",
    "# Save oof predictions\n",
    "logits, _, metrics = trainer.predict(val_ds)\n",
    "if isinstance(logits, tuple):\n",
    "    logits = logits[0]\n",
    "\n",
    "oof_dict = {\"id\": val_df[\"id\"], \"logits\": logits}\n",
    "save_pickle(oof_dict, OUT_DIR/f\"{EXP_NAME}_oof.pkl\")\n",
    "\n",
    "fin_f1_score = np.round(metrics[\"test_f1\"], 6)\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print(f\"  EXP {EXP_NAME}, F1-SCORE: {fin_f1_score}\")\n",
    "print(\"=\" * 30)\n",
    "if cfg.wandb:\n",
    "    wandb.log({\"cv\": fin_f1_score})\n",
    "\n",
    "# save experiment config file\n",
    "config_file_save_path = OUT_DIR / f\"{EXP_NAME}_config.yaml\"\n",
    "with open(config_file_save_path, \"w\") as fp:\n",
    "    OmegaConf.save(config=cfg, f=fp.name)\n",
    "shutil.copyfile(config_file_save_path, os.path.join(wandb.run.dir, f\"{EXP_NAME}_config.yaml\"))\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018005,
     "end_time": "2022-11-20T00:59:14.021706",
     "exception": false,
     "start_time": "2022-11-20T00:59:14.003701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.215633,
     "end_time": "2022-11-20T00:59:15.258915",
     "exception": false,
     "start_time": "2022-11-20T00:59:14.043282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(ROOT_DIR/'input'/'test.csv')\n",
    "sub_df = pd.read_csv(ROOT_DIR/'input'/'sample_submission.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.0626,
     "end_time": "2022-11-20T00:59:17.336512",
     "exception": false,
     "start_time": "2022-11-20T00:59:15.273912",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ds = preprocess_data(test_df, mode=\"inference\")\n",
    "# sort test dataset to have similar length samples in a batch to speed up inference\n",
    "test_ds = test_ds.sort(\"length\")\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 183.3963,
     "end_time": "2022-11-20T01:02:29.119814",
     "exception": false,
     "start_time": "2022-11-20T00:59:25.723514",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = ModelConfig(model_name_or_path=cfg.model.model_name_or_path)\n",
    "model = DataSolveModel(model_config)\n",
    "model.load_state_dict(torch.load(OUT_DIR/'pytorch_model.bin'))\n",
    "trainer_args = TrainingArguments(\"./tmp\", per_device_eval_batch_size = cfg.training_args.per_device_eval_batch_size)\n",
    "trainer = Trainer(model, trainer_args, data_collator=DataCollatorWithPadding(tokenizer))\n",
    "out = trainer.predict(test_ds)\n",
    "logits = out.predictions[0] if isinstance(out.predictions, tuple) else out.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014672,
     "end_time": "2022-11-20T01:02:29.14982",
     "exception": false,
     "start_time": "2022-11-20T01:02:29.135148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.868639,
     "end_time": "2022-11-20T01:02:30.033557",
     "exception": false,
     "start_time": "2022-11-20T01:02:29.164918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "for id_ in test_ds['id']:\n",
    "    for col in LABEL_COLS:\n",
    "        ids.append(f\"{id_}_{col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.731094,
     "end_time": "2022-11-20T01:02:30.780405",
     "exception": false,
     "start_time": "2022-11-20T01:02:30.049311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = post_process_logits(logits, threshold=0.5)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.953533,
     "end_time": "2022-11-20T01:02:31.757434",
     "exception": false,
     "start_time": "2022-11-20T01:02:30.803901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df[\"id\"] = ids\n",
    "sub_df['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.715069,
     "end_time": "2022-11-20T01:02:32.488009",
     "exception": false,
     "start_time": "2022-11-20T01:02:31.77294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 14.850622,
     "end_time": "2022-11-20T01:02:47.357277",
     "exception": false,
     "start_time": "2022-11-20T01:02:32.506655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(OUT_DIR/f\"{EXP_NAME}_sub.csv\", index=False)\n",
    "test_logits_dict = {\"id\":  test_ds['id'], \"logits\": logits}\n",
    "save_pickle(test_logits_dict, OUT_DIR/f\"{EXP_NAME}_test_logits.pkl\")\n",
    "if cfg.wandb:\n",
    "    # log artifacts to wandb\n",
    "    if cfg.upload_artifacts_to_wandb:\n",
    "        model_artifact = wandb.Artifact(name=EXP_NAME, type=\"model\")\n",
    "        model_artifact.add_dir(OUT_DIR)\n",
    "        wandb.log_artifact(model_artifact)\n",
    "\n",
    "    wandb.alert(\n",
    "        title=f\"Experiment {EXP_NAME}\",\n",
    "        text=f\"ðŸŽ‰ Finished experiment {EXP_NAME}, Score: {fin_f1_score}\",\n",
    "        level=AlertLevel.INFO,\n",
    "        wait_duration=0,\n",
    "    )\n",
    "\n",
    "    wandb.save(f\"{EXP_NAME}_sub.csv\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015329,
     "end_time": "2022-11-20T01:02:47.388235",
     "exception": false,
     "start_time": "2022-11-20T01:02:47.372906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.014869,
     "end_time": "2022-11-20T01:02:47.418166",
     "exception": false,
     "start_time": "2022-11-20T01:02:47.403297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ENVIRON == \"kaggle\":\n",
    "    shutil.rmtree(\"./tmp\", ignore_errors=True)\n",
    "    shutil.rmtree(ROOT_DIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crimson-elevator-29/crimson-elevator-29_test_logits.pkl\", \"rb\") as handler:\n",
    "\n",
    "    logits = pickle.load(handler)['logits']\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_stu = []\n",
    "for l in logits:\n",
    "    probs = [[p, 1-p] for p in l]\n",
    "    logits_stu.append(probs)\n",
    "logits_stu\n",
    "# logits_stu = torch.Tensor(logits_stu)\n",
    "# logits_tea = torch.Tensor(logits_stu)\n",
    "# sum(torch.nn.KLDivLoss()(logits_stu[:, i], logits_tea[:, i]) for i in range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_stu = model(**inputs)\n",
    "# extract bce loss and logits from student\n",
    "loss_stu = outputs_stu.loss\n",
    "logits_stu = outputs_stu.logits\n",
    "# extract logits from teacher\n",
    "with torch.no_grad():\n",
    "    outputs_tea = self.teacher_model(**inputs)\n",
    "    logits_tea = outputs_tea.logits\n",
    "# soften probabilites and compute distillation loss\n",
    "kdl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "# temperature\n",
    "logits_stu = torch.sigmoid(logits_stu) / self.args.temperature\n",
    "logits_tea = torch.sigmoid(logits_tea) / self.args.temperature\n",
    "\n",
    "loss_kd = self.args.temperature ** 2 \\\n",
    "            * sum(kdl_loss(logits_stu[idx], logits_tea[idx]) for idx in range(50)) \n",
    "# return weighted student loss\n",
    "loss = self.args.alpha * loss_stu + (1. - self.args.alpha) * loss_kd\n",
    "return (loss, outputs_stu) if return_outputs else loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "name": "kaggle.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
